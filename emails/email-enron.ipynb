{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to use enron email data from Kaggle to search using keyword, semantic search and vector search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Parse Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, email\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file (only 1000 rows for now)\n",
    "file_path = 'enronemails.csv'\n",
    "emails_df = pd.read_csv(file_path, nrows=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse the email content\n",
    "def parse_email(message):\n",
    "    from email.parser import Parser\n",
    "\n",
    "    email = Parser().parsestr(message)\n",
    "    return {\n",
    "        \"From\": email.get('From'),\n",
    "        \"To\": email.get('To'),\n",
    "        \"Subject\": email.get('Subject'),\n",
    "        \"Date\": email.get('Date'),\n",
    "        \"Body\": email.get_payload()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each row in the dataframe\n",
    "parsed_emails = emails_df['message'].apply(parse_email)\n",
    "parsed_emails_df = pd.DataFrame(parsed_emails.tolist())\n",
    "\n",
    "# Merge the parsed email details with the original dataframe\n",
    "emails_df = emails_df.join(parsed_emails_df)\n",
    "\n",
    "# Display the first few rows of the updated dataframe\n",
    "emails_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove rows with no subject or 'Test' subject\n",
    "emails_df = emails_df[emails_df['Subject'] != '']\n",
    "emails_df = emails_df[emails_df['Subject'] != 'test']\n",
    "print(len(emails_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -> Keyword Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "def search_email_chain_by_keyword(dataframe: pd.DataFrame, keyword: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Searches for a keyword in the email body and returns a chronological list of\n",
    "    people who used the keyword throughout an email chain.\n",
    "\n",
    "    Args:\n",
    "    dataframe (pd.DataFrame): The dataframe containing the email data.\n",
    "    keyword (str): The keyword to search for.\n",
    "\n",
    "    Returns:\n",
    "    List[Dict]: A chronological list of people who used the keyword in the email chain.\n",
    "    \"\"\"\n",
    "    # Filter the dataframe for rows containing the keyword in the Body\n",
    "    keyword_filtered = dataframe[dataframe['Body'].str.contains(keyword, case=False, na=False)]\n",
    "\n",
    "    # Sort the result by Date\n",
    "    keyword_filtered_sorted = keyword_filtered.sort_values(by='Date')\n",
    "\n",
    "    # Extract relevant information\n",
    "    email_chain = []\n",
    "    for _, row in tqdm(keyword_filtered_sorted.iterrows(), desc=\"Searching Emails\"):\n",
    "        email_info = {\n",
    "            \"From\": row['From'],\n",
    "            \"To\": row['To'],\n",
    "            \"Date\": row['Date'],\n",
    "            \"Subject\": row['Subject'],\n",
    "            \"Body\": row['Body']\n",
    "        }\n",
    "        email_chain.append(email_info)\n",
    "\n",
    "    return email_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keyword search\n",
    "search_results_chain = search_email_chain_by_keyword(emails_df, \"Looking for market trends\")\n",
    "search_results_chain[:5] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Search using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess(text):\n",
    "    # Preprocess and tokenize emails and query\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return [word for word in tokens if word.isalpha() and word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search_email_body(dataframe, query, top_n=5):\n",
    "    processed_emails = [' '.join(preprocess(email)) for email in tqdm(dataframe['Body'].fillna(\"\"), desc=\"Processing emails\")]\n",
    "    processed_query = ' '.join(preprocess(query))\n",
    "\n",
    "    # Create TF-IDF vectors\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(processed_emails + [processed_query])\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    cosine_similarities = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1]).flatten()\n",
    "\n",
    "    # Get top N similar emails\n",
    "    top_indices = cosine_similarities.argsort()[-top_n:][::-1]\n",
    "\n",
    "    # Return the top N similar emails\n",
    "    return dataframe.iloc[top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Example usage\n",
    "search_results_semantic = semantic_search_email_body(emails_df, \"Looking for market trends\", top_n=3)\n",
    "search_results_semantic['Body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "search_results_semantic = semantic_search_email_body(emails_df, \"Intellectual Prpperty\", top_n=3)\n",
    "search_results_semantic['Body']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search by using embeddings created by Open AI. Embeddings are stored in Pgvector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions for Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Helper function: calculate cost of embedding num_tokens\n",
    "# Assumes we're using the text-embedding-ada-002 model\n",
    "# See https://openai.com/pricing\n",
    "def get_embedding_cost(num_tokens):\n",
    "    return num_tokens/1000*0.0001\n",
    "    \n",
    "# Helper function: calculate total cost of embedding all content in the dataframe\n",
    "def get_total_embeddings_cost():\n",
    "    total_tokens = 0\n",
    "    for i in range(len(emails_df.index)):\n",
    "        text = emails_df['Body'][i]\n",
    "        token_len = num_tokens_from_string(text)\n",
    "        total_tokens = total_tokens + token_len\n",
    "    total_cost = get_embedding_cost(total_tokens)\n",
    "    return total_cost\n",
    "\n",
    "# Helper func: calculate number of tokens\n",
    "def num_tokens_from_string(string: str, encoding_name = \"cl100k_base\") -> int:\n",
    "    if not string:\n",
    "        return 0\n",
    "    # Returns the number of tokens in a text string\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since emails are deleted, lets recreate the Index to avoid index error\n",
    "emails_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check on total token amount for price estimation\n",
    "total_cost = get_total_embeddings_cost()\n",
    "print(\"estimated price to embed this content = $\" + str(total_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Get openAI api key by reading local .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n",
    "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
    "\n",
    "embeddings = []\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\", max_length=2048):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    \n",
    "    #Split the text into chunks based on max token length\n",
    "    chunks = [text[i:i + max_length] for i in range(0, len(text), max_length)]\n",
    "\n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            # Make the API call to get embeddings for the chunk        \n",
    "            response = client.embeddings.create(\n",
    "                input=chunk, \n",
    "                model=model\n",
    "            )\n",
    "            embeddings.append(response.data[0].embedding)\n",
    "        except OpenAI.error.OpenAIError as e:\n",
    "            print(f\"An error occurred: {e}\")    \n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# text = 'Here is our forecast' \n",
    "# embedding = get_embedding(text)\n",
    "# embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings for each piece of content\n",
    "tqdm.pandas(desc=\"Processing\")\n",
    "\n",
    "emb_values = []\n",
    "for body in tqdm(emails_df['Body'], desc=\"Processing rows\"):\n",
    "    try:\n",
    "        emb_values.append(get_embedding(body))\n",
    "    # except BadRequestError as e:\n",
    "    #     emb_values.append('')\n",
    "    #     # Handle the error, e.g., by reducing the prompt size and retrying\n",
    "    #     print(\"The request was too long:\", e)\n",
    "    except Exception as e:\n",
    "        emb_values.append('')\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "# Assigning the results back to the new column\n",
    "emails_df['Embeddings'] = emb_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store Embeddings and Email data in Pgvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe from the list\n",
    "interested_columns_df = emails_df[['From', 'To', 'Subject', 'Date', 'Body', 'Embeddings']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(interested_columns_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect and configure pgvector\n",
    "connection_string  = os.environ['LOCAL_POSTGRESS_CONNECTION_STRING']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pgvector\n",
    "import math\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "from pgvector.psycopg2 import register_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to PostgreSQL database in Timescale using connection string\n",
    "conn = psycopg2.connect(connection_string)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install pgvector \n",
    "cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector\");\n",
    "conn.commit()\n",
    "\n",
    "# Register the vector type with psycopg2\n",
    "register_vector(conn)\n",
    "\n",
    "# Create table to store embeddings and metadata\n",
    "table_create_command = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS enron_embeddings (\n",
    "            id bigserial primary key, \n",
    "            EmailFrom text,\n",
    "            EmailTo text,\n",
    "            EmailSubject text,\n",
    "            EmailDate date,\n",
    "            EmailBody text,\n",
    "            Embedding vector(1536)\n",
    "            );\n",
    "            \"\"\"\n",
    "\n",
    "cur.execute(table_create_command)\n",
    "cur.close()\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_vector(conn)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from psycopg2.extras import execute_values\n",
    "from email.utils import parsedate_tz, mktime_tz\n",
    "from tqdm import tqdm\n",
    "\n",
    "def batch_insert_generator(df, batch_size=100):\n",
    "    batch = []\n",
    "    for index, row in df.iterrows():\n",
    "        batch.append((\n",
    "            row['From'],\n",
    "            row['To'],\n",
    "            row['Subject'],\n",
    "            str(datetime.fromtimestamp(mktime_tz(parsedate_tz(row['Date'])))),\n",
    "            row['Body'],\n",
    "            row['Embeddings'].tolist() if isinstance(row['Embeddings'], np.ndarray) else row['Embeddings']\n",
    "        ))\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    if batch:\n",
    "        yield batch\n",
    "\n",
    "# Define batch size\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# Use tqdm to show progress bar\n",
    "for batch in tqdm(batch_insert_generator(interested_columns_df, BATCH_SIZE), desc=\"Inserting batches\"):\n",
    "    execute_values(cur, \"\"\"\n",
    "    INSERT INTO enron_embeddings (EmailFrom, EmailTo, EmailSubject, EmailDate, EmailBody, Embedding)\n",
    "    VALUES %s\n",
    "    \"\"\", batch)\n",
    "    conn.commit()  # Commit after each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from email.utils import parsedate_tz, mktime_tz\n",
    "\n",
    "#Batch insert embeddings and metadata from dataframe into PostgreSQL database\n",
    "data_list = [(row['From'], row['To'], row['Subject'], str(datetime.fromtimestamp(mktime_tz(parsedate_tz(row['Date'])))), \n",
    "              row['Body'], \n",
    "              row['Embeddings'].tolist() if isinstance(row['Embeddings'], np.ndarray) else row['Embeddings']) \n",
    "             for index, row in tqdm(interested_columns_df.iterrows(), total=interested_columns_df.shape[0], desc=\"Inserting rows\")]\n",
    "\n",
    "# Use execute_values to perform batch insertion\n",
    "execute_values(cur, \"INSERT INTO enron_embeddings (EmailFrom, EmailTo, EmailSubject, EmailDate, EmailBody, Embedding) VALUES %s\", data_list)\n",
    "# Commit after we insert all embeddings\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"SELECT COUNT(*) as cnt FROM enron_embeddings;\")\n",
    "num_records = cur.fetchone()[0]\n",
    "print(\"Number of vector records in table: \", num_records,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an index on the data for faster retrieval\n",
    "\n",
    "#calculate the index parameters according to best practices\n",
    "num_lists = num_records / 1000\n",
    "if num_lists < 10:\n",
    "    num_lists = 10\n",
    "if num_records > 1000000:\n",
    "    num_lists = math.sqrt(num_records)\n",
    "\n",
    "#use the cosine distance measure, which is what we'll later use for querying\n",
    "cur.execute(f'CREATE INDEX ON enron_embeddings USING ivfflat (Embedding vector_cosine_ops) WITH (lists = {num_lists});')\n",
    "conn.commit() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> Search Pgvector using KNN operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: Get top 3 most similar documents from the database\n",
    "def get_top3_similar_emails(query_embedding, conn):\n",
    "    embedding_array = np.array(query_embedding)\n",
    "    # Register pgvector extension\n",
    "    register_vector(conn)\n",
    "    cur = conn.cursor()\n",
    "    # Get the top 3 most similar documents using the KNN <=> operator\n",
    "    #To_CHAR(EmailDate, 'YY/MM/DD'), \n",
    "    cur.execute(\"SELECT EmailFrom, EmailTo, EmailDate, EmailSubject, EmailBody FROM enron_embeddings ORDER BY Embedding <=> %s LIMIT 3\", (embedding_array,))\n",
    "    top3_docs = cur.fetchall()\n",
    "    return top3_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = 'Looking for market trends'\n",
    "related_emails = get_top3_similar_emails(get_embedding(user_input), conn)\n",
    "for item in related_emails:\n",
    "    print(f\"-->{item[4]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = 'Payment'\n",
    "related_emails = get_top3_similar_emails(get_embedding(user_input), conn)\n",
    "for item in related_emails:\n",
    "    print(f\"-->{item[4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions to save in json.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Keys for the JSON objects\n",
    "# import json\n",
    "# keys = ['from', 'to', 'title', 'content']\n",
    "\n",
    "# # Convert each tuple to a dictionary and add it to the list\n",
    "# json_list = [dict(zip(keys, t)) for t in related_emails]\n",
    "\n",
    "# json_str = json.dumps(json_list, indent=4)\n",
    "\n",
    "# # Save the JSON string to a file\n",
    "# with open('filtered_emails.json', 'w') as file:\n",
    "#     file.write(json_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the column names to create dataframe for\n",
    "column_names = ['from', 'to', 'date', 'title', 'content']\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(related_emails, columns=column_names)\n",
    "\n",
    "random_integers = np.random.randint(1, 100, size=len(df))\n",
    "#insert type and id in the begining\n",
    "df.insert(0, 'type', 'email')\n",
    "df.insert(1, 'id', random_integers)\n",
    "\n",
    "# Convert the datetime.date objects to strings\n",
    "df['date'] = df['date'].apply(lambda x: x.strftime('%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Convert the DataFrame to a list of dictionaries\n",
    "json_list = df.to_dict(orient='records')\n",
    "json_str = json.dumps(json_list, indent=4)\n",
    "\n",
    "print(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the JSON string to a file\n",
    "with open('filtered_emails.json', 'w') as file:\n",
    "    file.write(json_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "some company",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
